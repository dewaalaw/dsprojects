{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted: '/Users/ad/Documents/Exercises/_RgressionModeling/data/signage2015_kloof_seapoint.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c3c208d5b8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/ad/Documents/Exercises/_RgressionModeling/data/signage2015_kloof_seapoint.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted: '/Users/ad/Documents/Exercises/_RgressionModeling/data/signage2015_kloof_seapoint.csv'"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv(\"/Users/ad/Documents/Exercises/_RgressionModeling/data/signage2015_kloof_seapoint.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is multivariate, containing continous and categorical variables, measured in different units. _Width_ and _Height_ are measured in metres, while _Area_ is measured in metres-sqaured. The sign's geodetic vertical and horizontal location relative to the prime meridian at Greenwich and naught at the equator is measured in _latitude_ and _longitude_ decimal degrees, respectively. The _illum_ feature maps _Illuminated_, hence only one of these are needed, noting that the variables represent a qualitative property, i.e., whether a sign is of type illuminated. Other categorical variables are 'Suburb' and 'sign'. Finally, illuminated signs are a sub-category of sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statitstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the logDimension and append frame with logValue\n",
    "def calcLog(dimension): \n",
    "    var = 'log'+ dimension\n",
    "    if var in df.columns: # Check if column exist in df\n",
    "        return var\n",
    "    else:\n",
    "        df[var] = np.log(df[dimension]+1)\n",
    "        return var\n",
    "    \n",
    "def reset_df():\n",
    "    return df[['sign', 'Illuminated', 'Height', \n",
    "               'Width', 'Area', 'longitude', \n",
    "               'latitude', 'Suburb', 'illum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7), dpi=123, facecolor='#E4E5E9') # Generate the figure\n",
    "spec = fig.add_gridspec(ncols=7, nrows=3)\n",
    "\n",
    "# Configure Distribution Plots\n",
    "g0 = fig.add_subplot(spec[0, :4])\n",
    "sns.histplot(df[calcLog('Height')], kde=True, color='#657FFB', edgecolor='black') #kde - kernal density estimate\n",
    "#sns.histplot(x='Height', data=df, kde=True, color='#657FFB', edgecolor='black') \n",
    "plt.ylabel('# of Signs', fontsize=11)\n",
    "plt.xlabel('Log Height', fontsize=11)\n",
    "plt.xlim(0, 3)\n",
    "plt.ylim(0, 80)\n",
    "\n",
    "g1 = fig.add_subplot(spec[1, :4])\n",
    "sns.histplot(df[calcLog('Width')], kde=True, color='#BAD2EF', edgecolor='black') \n",
    "#sns.histplot(x='Width', data=df, kde=True, color='#BAD2EF', edgecolor='black')\n",
    "plt.ylabel('# of Signs', fontsize=11)\n",
    "plt.xlabel('Log Width', fontsize=11)\n",
    "#plt.xlim(0, 20)\n",
    "plt.ylim(0, 80)\n",
    "\n",
    "g2 = fig.add_subplot(spec[2, :4])\n",
    "sns.histplot(df[calcLog('Area')], kde=True, color='#657FFB', edgecolor='black') \n",
    "#sns.histplot(x='Area', data=df, kde=True, color='#BAD2EF', edgecolor='black')\n",
    "plt.ylabel('# of Signs', fontsize=11)\n",
    "plt.xlabel('Log Area', fontsize=11)\n",
    "#plt.xlim(0, 20)\n",
    "plt.ylim(0, 80)\n",
    "\n",
    "# Configure Box Plots\n",
    "g3 = fig.add_subplot(spec[:, 4])\n",
    "sns.boxplot(y=df['Height'], color='#657FFB')\n",
    "plt.xlabel('Height', labelpad=5, fontsize=10)\n",
    "plt.ylabel(' ')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "g4 = fig.add_subplot(spec[:, 5])\n",
    "sns.boxplot(y=df['Width'], color='#BAD2EF')\n",
    "plt.xlabel('Width', labelpad=5, fontsize=10)\n",
    "plt.ylabel(' ')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "g5 = fig.add_subplot(spec[:, 6])\n",
    "sns.boxplot(y=df['Area'], color='#657FFB')\n",
    "plt.xlabel('Area', labelpad=5, fontsize=10)\n",
    "plt.ylabel(' ')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "for g in [g0, g1, g2, g3, g4, g5]:\n",
    "    g.patch.set_alpha(0.0)\n",
    "    g.spines['top'].set_visible(False)\n",
    "    g.spines['right'].set_visible(False)\n",
    "    \n",
    "fig.suptitle('Distribution Sign Dimensionsin meters and meters squared', fontsize=15, y=1)\n",
    "plt.show()\n",
    "\n",
    "df = reset_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 7), dpi=123, facecolor='#E4E5E9') # Generate the figure\n",
    "spec = fig.add_gridspec(ncols=6, nrows=3)\n",
    "\n",
    "g0 = fig.add_subplot(spec[:, 0:2])\n",
    "sns.boxplot(x=df['Illuminated'], y=df['Height'], color='#BAD2EF')\n",
    "plt.xlabel('Illuminted (Height)', labelpad=5, fontsize=10)\n",
    "plt.ylabel('Height')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "g1 = fig.add_subplot(spec[:, 2:4])\n",
    "sns.boxplot(x=df['Illuminated'], y=df['Width'], color='#BAD2EF')\n",
    "plt.xlabel('Illuminted (Width)', labelpad=5, fontsize=10)\n",
    "plt.ylabel('Width')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "g2 = fig.add_subplot(spec[:, 4:6])\n",
    "sns.boxplot(x=df['Illuminated'], y=df['Area'], color='#BAD2EF')\n",
    "plt.xlabel('Illuminted (Area)', labelpad=5, fontsize=10)\n",
    "plt.ylabel('Area')\n",
    "plt.ylim(0, 35)\n",
    "\n",
    "for g in [g0, g1, g2, g3]:\n",
    "    g.patch.set_alpha(0.0)\n",
    "    g.spines['top'].set_visible(False)\n",
    "    g.spines['right'].set_visible(False)\n",
    "    \n",
    "fig.suptitle('Box Plots for Illuminated Signs Dimensions', fontsize=15, y=1)\n",
    "plt.show()\n",
    "\n",
    "df = reset_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain descriptive statics of continous variables\n",
    "df[['Height','Width', 'Area']].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df[['Height','Width', 'Area', 'Suburb']].groupby('Suburb')\n",
    "grouped.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plots and the frequency distributions, the continuous variables (Height, Width, and Area) are skewed to the right, and several outliers beset the data. However, rather than a measurement error, these are outsized signs. Given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Korstanje, J. (2022)., p. 267. Machine Learning on Geographical Data Using Python.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(20, 500))\n",
    "df[['MarkerSize']] = scaler.fit_transform(df[['Area']])\n",
    "\n",
    "plt.figure(figsize=(14, 7), dpi=123, facecolor='#E4E5E9')\n",
    " \n",
    "plt.scatter(df[df.Suburb == 'Gardens']['longitude'], \n",
    "            df[df.Suburb == 'Gardens']['latitude'], \n",
    "            s=df[df.Suburb == 'Gardens']['MarkerSize'], \n",
    "            c='none', \n",
    "            edgecolors='black')\n",
    "plt.xlabel('Longitude', labelpad=5, fontsize=10)\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Relative Sign Area - Gardens', fontsize=15, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7), dpi=123, facecolor='#E4E5E9')\n",
    "\n",
    "plt.scatter(df[df.Suburb == 'Seapoint']['longitude'], \n",
    "            df[df.Suburb == 'Seapoint']['latitude'], \n",
    "            s=df[df.Suburb == 'Seapoint']['MarkerSize'], \n",
    "            c='none', \n",
    "            edgecolors='black')\n",
    "plt.xlabel('Longitude', labelpad=5, fontsize=10)\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Relative Sign Area - Seapoint', fontsize=15, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily\n",
    "\n",
    "# Generate scatter plot\n",
    "joint_axes = seaborn.jointplot(\n",
    "    x='longitude', y='latitude', data=df[df.Suburb == 'Gardens'], s=0.5\n",
    ")\n",
    "contextily.add_basemap(\n",
    "    joint_axes.ax_joint,\n",
    "    crs=\"EPSG:4326\",\n",
    "    source=contextily.providers.CartoDB.PositronNoLabels\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatter plot\n",
    "import seaborn\n",
    "seaborn.jointplot(x='longitude', y='latitude', data=df[df.Suburb == 'Gardens'], s=0.5);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seaborn.jointplot(x='longitude', y='latitude', data=df[df.Suburb == 'Seapoint'], s=0.5);\n",
    "#plt.figure(figsize=(14, 7), dpi=123, facecolor='#E4E5E9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Suburb == 'Gardens']['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardens = df[df.Suburb == 'Gardens']\n",
    "gardens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform (or Scale) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some machine learning algorithms (e.g., neural networks and SVM's) are sensitive to outliers and non-normally distributed data (Müller et al., 2017, p. 132), or others (e.g., K-means and K-Nearest Neighbors) that are sensitive to the scale of numeric features, requiring the data to be transformed, in the main, machine learning algorithms learn the distribution embedded within the features themselves (Mueller et al., 2016, p. 226). According to Mueller (2016), transforming the data nonetheless allows for faster algortihm convergence and minimises prediction error.\n",
    "\n",
    "One way to addresss the above outliers, those outside the interquartile range, is the use of a scaler impervious to outliers. RobustScaler, as the name suggest, use statistics that are robust to outliers (Pedregosa et al., JMLR 12, pp. 2825-2830, 2011; Buitinck et al., 2013).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale continous variables, except coordinates (lat, long)\n",
    "features_to_scale = ['Width', 'Height', 'Area']\n",
    "dat_to_scale = df[features_to_scale]\n",
    "\n",
    "# RobustScaler's desensitised to outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scale = RobustScaler()\n",
    "robust_scale.fit(dat_to_scale)\n",
    "\n",
    "dat_to_scale = robust_scale.transform(dat_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_to_scale = pd.DataFrame(dat_to_scale, columns=features_to_scale)\n",
    "dat_to_scale.head()\n",
    "\n",
    "df_scaled = df\n",
    "df_scaled = df_scaled.assign(Width=dat_to_scale['Width'])\n",
    "df_scaled = df_scaled.assign(Height=dat_to_scale['Height'])\n",
    "df_scaled = df_scaled.assign(Area=dat_to_scale['Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataframe\n",
    "#X_train_scale = X_train\n",
    "\n",
    "# Reassign transformed values to X_train\n",
    "#X_train_scale = X_train_scale.assign(Width=dat_to_scale[:, 0], \n",
    "#                               Height=dat_to_scale[:, 1],\n",
    "#                               Area=dat_to_scale[:, 2])\n",
    "\n",
    "# Repeat for test sample\n",
    "#dat_to_scale = X_test[features_to_scale]\n",
    "#dat_to_scale = robust_scale.transform(dat_to_scale) \n",
    "\n",
    "#X_test_scale = X_test\n",
    "#X_test_scale = X_test_scale.assign(Width=dat_to_scale[:, 0],\n",
    "#                                   Height=dat_to_scale[:, 1],\n",
    "#                                   Area=dat_to_scale[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Enconding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification algorithms, one feature for each qualitative ascription is prefered (Müller et al., 2017). This is achieved by the one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ferret out the needed columns\n",
    "feature_col = ['Suburb', 'sign', 'illum', 'Width', 'Height', 'Area', 'latitude', 'longitude']\n",
    "\n",
    "# One-Hot-Encoding\n",
    "df_onehot = pd.get_dummies(df_scaled[feature_col])\n",
    "\n",
    "# Ferret out new variable names\n",
    "feature_col_onehot = df_onehot.loc[:, 'Width':'sign_projctng'].columns\n",
    "\n",
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature (X) and target (y) extraction\n",
    "X = df_onehot[feature_col_onehot] # Features\n",
    "y = df_onehot['illum_Yes'] # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition train and test samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the descriptive statistics summary and concommitant plots, the data contains several outliers, and are non-normally distributed insofar positvely skewed. Accordingly, the data normality assumption is unmet (i.e., the assumption that the data is obtained from a normally distributed population—Halswanter, 2016, p.97), thus a learning method that makes use of non-parametric inferences would better suit classification. Decision Tree Classifiers are non-parametric supervised machine learners that develop classification rules based on if-then-else desiontry. They are suitable for features measured at different scales, that are nominal, and continous (Müller 2017, p. 83). However, Decisiontree's tend to overfit to the training data.\n",
    "\n",
    "To counter an overfit, the maximum depth (i.e., the number of consecutive questions, denoted by the number of branching layers) of the tree is controlled. Whereas setting the maximum tree depth is known as pre-pruning, post-pruning encompass dropping or removing uninformative nodes. Below we compare the variation of maximum-depth to model's predictive accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "maxdepth = []\n",
    "accuracy = []\n",
    "crossvalscoreavg = []\n",
    "modelscore = []\n",
    "\n",
    "for max_depth in range(1, 15):\n",
    "    tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    maxdepth.append(max_depth)\n",
    "    \n",
    "    scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "    crossvalscoreavg.append(scores.mean())\n",
    "    \n",
    "    y_pred = cross_val_predict(tree, X_test, y_test, cv=5)\n",
    "    \n",
    "    accscore = accuracy_score(y_test, y_pred)\n",
    "    accuracy.append(accscore)\n",
    "    \n",
    "    modscore = tree.score(X_test, y_test)\n",
    "    modelscore.append(modscore)\n",
    "    \n",
    "    print(\"max_depth: {}, accuracy: {:.3f}, avg_crossval: {:.3f}, model_score: {:.3f}\".format(max_depth, \n",
    "                                                                                          accscore, \n",
    "                                                                                          scores.mean(), \n",
    "                                                                                          modscore))\n",
    "\n",
    "plt.plot(maxdepth, crossvalscoreavg, linestyle=\"dotted\", label=\"cross_val_score\")\n",
    "plt.plot(maxdepth, accuracy, linestyle=\"dashed\", label=\"accuracy_score\")\n",
    "plt.plot(maxdepth, modelscore, linestyle=\"dashdot\", label=\"Model Score (Test Set)\")\n",
    "\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "#plt.ylabel(\"Measure\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Average Cross-Validation score measure the model's generalisation performance in terms of its probability of accurate predicitions on average. From the graph, at depths greater 4,  the average score improves, petering at around 60% accuracy. \n",
    "\n",
    "This trend translates to the Model Test Score, i.e., "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum tree depth is compared to accuracy score. We find the highest accuracy (0.575) and cross validation (0.602) is at maximum depth 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]\n",
    "max_depth_range = _range\n",
    "min_samples_leaf_range = _range\n",
    "param_grid = [{'criterion': ['entropy', 'gini'], \n",
    "               'max_depth': max_depth_range},\n",
    "              {'min_samples_leaf': min_samples_leaf_range}]\n",
    "grid_searchCV = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, grid search is ran over two sets of parameters, first with every combination of `criterion` and `max_depth` and second, for `min_samples_leaf` (ref.:Stackoverflow).\n",
    "\n",
    "REF: https://stackoverflow.com/questions/38709690/scikit-learn-using-gridsearchcv-on-decisiontreeclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searchCV.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.3f}\".format(grid_searchCV.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid_searchCV.best_params_))\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_searchCV.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_searchCV.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'criterion': ['entropy', 'gini'], \n",
    "               'max_depth': max_depth_range},\n",
    "              {'min_samples_leaf': min_samples_leaf_range}]\n",
    "# GridSearch with CV Splitter\n",
    "grid_searchCV = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=strat_shuf_split, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searchCV.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.3f}\".format(grid_searchCV.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid_searchCV.best_params_))\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_searchCV.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_searchCV.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'criterion': ['entropy', 'gini'], \n",
    "               'max_depth': max_depth_range}]\n",
    "\n",
    "grid_searchCV = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=strat_shuf_split, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searchCV.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.3f}\".format(grid_searchCV.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid_searchCV.best_params_))\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid_searchCV.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_searchCV.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Grid Search CV Model| Test Score | Best Parmaters| Best Cross Val Score |  \n",
    "|--------------------|------------|---------------|----------------------|\n",
    "|CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(tree, X, y, cv=11, scoring='f1')\n",
    "print(scores)\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(tree, X, y, cv=11)\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "maxdepth = []\n",
    "accuracy = []\n",
    "for max_depth in range(1,14):\n",
    "    tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth).fit(X_train, y_train)\n",
    "    predict = tree.predict(X_test)\n",
    "    maxdepth.append(max_depth)\n",
    "    accuracy.append(accuracy_score(y_test, predict))\n",
    "    #print(\"max_depth {}, accuracy {:.3f}\".format(max_depth, accuracy_score(y_test, predict)))\n",
    "\n",
    "plt.plot(maxdepth, accuracy, linestyle=\"--\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_size, train_scores, test_scores = learning_curve(tree,\n",
    "                                                      X, y,\n",
    "                                                      cv=11,\n",
    "                                                      scoring='f1',\n",
    "                                                      n_jobs = -1, #Use all the processors\n",
    "                                                      train_sizes = np.linspace(0.01, 1, 100),\n",
    "                                                      verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "print(train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = np.mean(test_scores, axis=1)\n",
    "print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_size, train_mean, label='Training Score')\n",
    "plt.plot(train_size, test_mean, label='Cross-Validation Score')\n",
    "\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0, max_depth=7).fit(X_train, y_train)\n",
    "# tree = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"\\nAccuracy Score:\", format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y_test, y_pred)))\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"\\nTest set score: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "print('Correlation Matrix: \\n{}'.format(correlation_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, mask=np.zeros_like(correlation_matrix, dtype=np.bool),\n",
    "           cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "           square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off at the sign dimentions, we find that \n",
    "- area has a higher correlation with height (0.813) than width (0.390); \n",
    "- compared to the four sign types (i.e., flat-, boundary-, canopy-, and projecting signs) Area has a higher correlation with flat signs (0.137), as does height (0.094) and width (0.314); \n",
    "- finally, of the three sign dimensions, area has a higher correlation with illuminated sign (0.060). \n",
    "\n",
    "Illuminated signs have a higher correlation with flat signs (0.057). Whereas Gardens have a higher correlation with boundary wall signs(0.153), Seapoint has a higher correlation with flat signs (0.146).\n",
    "Interestingly, while all signs are negatively correlated, flat sign have the greatest negative correlation with canopy sign (-0.666). A possible reason for this could be where there is no canopy over the pavement, eliminating the choice of suspending a sign from the canopy; flatsigns being the next best option. Here building dimensionality is a case in point for determining sign choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, Features with an absolute correlation greater 0.5 threshold were selected. These"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_feature_matrix = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.6:\n",
    "            corr_col = correlation_matrix.columns[i]\n",
    "            correlated_feature_matrix.add(corr_col)\n",
    "\n",
    "            \n",
    "print(correlated_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with one-hot-encoded data\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"\\nAccuracy Score: {:.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y_test, y_pred)))\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"\\nTest set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can conclude that:\n",
    "- the number of signs correctly classified as illuminated is 36 (i.e., True Positive);\n",
    "- the number of signs correctly classified as un-illimunated is 59 (i.e., True Negative);\n",
    "- the number of un-illuminated signs classified as illuminated is 20 (i.e., False Positive);\n",
    "- and the number of illuminated signs wrongly classified as un-illuminated is 65 (i.e., False Negative)\n",
    "\n",
    "Accuracy Score = Correct Predictions / Total Predictions. Or the number of successfull predictions:\n",
    "(True Positives + True Negatives) / (Total in test sample) = (36 + 59) / (0.25 * 720) = 0.528\n",
    "\n",
    "Recall Score (or true positive rate) = Percentage of illuminated signs identified correctly:\n",
    "(True Positive) / (True Positve + False Negative) = 36 / (36 + 65) = 0.356\n",
    "\n",
    "Precision Score = Percentage of signs identified as illuminated that are actually illuminated:\n",
    "(True Positive) / (True Positive + False Positive) = 36 / (36 + 20) = 0.643"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scaled = LogisticRegression().fit(X_train_scale, y_train)\n",
    "y_pred = logreg_scaled.predict(X_test_scale)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"\\nAccuracy Score:\", format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y_test, y_pred)))\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled.score(X_train_scale, y_train)))\n",
    "print(\"\\nTest set score: {:.3f}\".format(logreg_scaled.score(X_test_scale, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data delivers slightly better training- and test set scores. However, the recall metric is below desired. As corroborated by the false negative count (i.e., 65), a large proportion of illuminated signs are wrongly classified.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that an illuminated sign comes at premium, a lower recall is undesirable say for a cost estimate. One way to adjust the tradeoff between recall and precision is to adjust the threshold at which the model makes the classification decision (Müller et al., 2017, p. 289). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_precision_recall_curve(precision, recall, thresholds, threslabel):\n",
    "    # Find Threshold closest to zero\n",
    "    close_zero = np.argmin(np.abs(thresholds))\n",
    "    plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "        label = threslabel, fillstyle='none', c='k', mew=2)\n",
    "\n",
    "    plt.plot(precision, recall, label='precision recall curve')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(loc=2)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, logreg_scaled.decision_function(X_test_scale))\n",
    "plot_precision_recall_curve(precision, recall, thresholds, 'threshold naught')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decision_function`'s default threshold is 0—that is the black circle in the *Precision-Recall* graph above—corresponding to 0.64 precision and 0.356 recall. The steep gradient at the beginning, between recall equalling 1 and 0.4, suggest that gain in precision sacrifices plenty recall. On the whole the model has low recall retention as precision increases. Prefered in this case is that recall is kept high as precision increases. That is, a graph that veers toward the upper-right corner, rather than (in the above case) one that recedes there-from (Müller et al., 2017, p. 290).  \n",
    "\n",
    "Given the graph's naught threshold position (with increasing threshold from left to right), setting the *operating-point* to increase recall would mean reducing the threshold below naught—i.e., shift the threshold point to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the threshold below naught\n",
    "y_pred_lower_threshold = logreg_scaled.decision_function(X_test_scale) > -0.8\n",
    "print(classification_report(y_test, y_pred_lower_threshold))\n",
    "\n",
    "print(\"\\nConfusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred_lower_threshold)))\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "print(\"\\nAverage precision {:.3f}\".format(average_precision_score(y_test, y_pred_lower_threshold)))\n",
    "\n",
    "plot_precision_recall_curve(precision, recall, y_pred_lower_threshold, 'adjusted threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although adjusting the threshold yields significant recall improvement (from 0.36 to 0.96 in the above graph), slightly decreasing precision (from 0.65 to 0.58), the number of false positives shot up (from 19 to 71). Further, the average precision—i.e., the area under the precision-recall curve—totals 0.577. This  is slightly off the midrange mark (0.5) for average precision (Müller et al., 2017, p. 292). For this reason, it's concluded that the model has average precsion.   \n",
    "\n",
    "Precision and recall are important meausures for determining model success, not least for 'class-imbalenced problems' (Chollet, 2018, p. 112). For 'balanced classification problems' the *receiver operating characteristc* (ROC) and *area under the curve* (AUC) metric are recommended. Before calculating these, class imbalance occur when 'one class is more frequent than the other', also called an imbalanced data set (Müller et al., 2017, p. 277). In this example the number of illuminated signs relatively equal the un-illuminated signs (352 illuminated out of 720 signs), suggesting that the ROC and AUC metric are better measures of model success (Chollet, 2018, p. 112).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot['illum_Yes'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve plots the *True Positive Rate* (TPR) or Recall on the y-axis against the *False Positive Rate* (FPR) on the x-axis at various thresholds. A suitable ROC curve yields high Recall and low FPR—a curve that traverses upwards from the origin toward the top left corner, petering out at the top right corner (Müller et al., 2017, p. 293). The AUC is the area under the ROC curve,  thus:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "logreg_auc = roc_auc_score(y_test, logreg_scaled.decision_function(X_test_scale))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg_scaled.decision_function(X_test_scale))\n",
    "\n",
    "def plot_custom_roc(fpr, tpr, thresholds):\n",
    "    plt.plot(fpr, tpr, label=\"ROC Curve (AUC = %0.3f)\" % logreg_auc)\n",
    "    plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "    plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "    \n",
    "    # Find threshold closest to zero\n",
    "    close_zero = np.argmin(np.abs(thresholds))\n",
    "    plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label = 'naught threshold @ (fpr = %0.3f, ' % fpr[close_zero] + 'tpr = %0.3f)' % tpr[close_zero], \n",
    "         fillstyle='none', c='k', mew=2)\n",
    "    \n",
    "    plt.plot([0, 1], linestyle=\":\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=4)\n",
    "\n",
    "plot_custom_roc(fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dud = pd.DataFrame()\n",
    "type(dud)\n",
    "\n",
    "box = []\n",
    "type(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naught (or default) threshold positions at 0.266 FPR and 0.356 TPR. The AUC (0.566) suggest that there's 56.6% 'probability that a randomly picked point of the positive class will have a higher score according to the classifier than a randomly picked point from the negative class' (Müller et al., 2017, p. 295). Hence the classifier  predicts slighty better than a random-guess predictor with AUC = 0.5 (i.e., the area under the diagonal line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg_cross_val = LogisticRegression(max_iter=200).fit(X_train_scale, y_train)\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=10, scoring='f1')\n",
    "print(\n",
    "    'Cross-validation scores (10 splits): \\n {}'.format(scores)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "\n",
    "#print('Cross Val Score: {}'.format(logreg_cross_val.score(X_test_scale, y_test)))\n",
    "#print('Average cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=10)\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=kfold, scoring='f1')\n",
    "print('Cross-validation scores (10 splits): \\n {}'.format(scores))\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "# print('\\nAverage cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "repeated_kfold = RepeatedKFold(n_splits=10, n_repeats=2)\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=repeated_kfold, scoring='f1')\n",
    "print('Cross-validation scores (10 splits): \\n {}'.format(scores))\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "# print('\\nAverage cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_shuffle = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=kfold_shuffle, scoring='f1')\n",
    "print('\\nCross-validation scores (10 splits, shuffle=True, random_state=0): \\n {}'.format(scores))\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "\n",
    "print('\\nAverage cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "shuffle_split = ShuffleSplit(n_splits=10, random_state=0)\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=shuffle_split, scoring='f1')\n",
    "print('\\nCross-validation scores (10 splits, shuffle=True, random_state=0): \\n {}'.format(scores))\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "\n",
    "print('\\nAverage cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "scores = cross_val_score(logreg_cross_val, X_train_scale, \n",
    "                         y_train, cv=skf, scoring='f1')\n",
    "print('\\nStratifiedKFold Cross-validation scores (10 splits, shuffle=True, random_state=0): \\n {}'.format(scores))\n",
    "print(\n",
    "    \"\\n%0.3f accuracy with a standard deviation of %0.3f\" \n",
    "    % (scores.mean(), scores.std())\n",
    ")\n",
    "\n",
    "print('\\nAverage cross-validation score: {:.3f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0, max_depth=7).fit(X_train, y_train)\n",
    "# tree = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"\\nAccuracy Score:\", format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y_test, y_pred)))\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"\\nTest set score: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "maxdepth = []\n",
    "accuracy = []\n",
    "for max_depth in range(1,14):\n",
    "    tree = DecisionTreeClassifier(random_state=0, max_depth=max_depth).fit(X_train_scale, y_train)\n",
    "    predict = tree.predict(X_test_scale)\n",
    "    maxdepth.append(max_depth)\n",
    "    accuracy.append(accuracy_score(y_test, predict))\n",
    "    #print(\"max_depth {}, accuracy {:.3f}\".format(max_depth, accuracy_score(y_test, predict)))\n",
    "\n",
    "plt.plot(maxdepth, accuracy, linestyle=\"--\")\n",
    "plt.xlabel(\"Maximum Depth\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.grid(True)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, tree.decision_function(X_test_scale))\n",
    "plot_custom_roc(fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_train_scale, y_train)\n",
    "\n",
    "plt.figure(figsize=(50,20))\n",
    "tree.plot_tree(tree_model, feature_names=X_train_scale.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg.fit(.coef_.T, '^', label=\"C=100\")\n",
    "plt.xticks(range(X.shape[1]), feature_cols_onehot, rotation=90)\n",
    "plt.hlines(0, 0, X.shape[1])\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.title(\"Coefieicients learned by logistic regression with L2 penalty for different C values, where C is the strength regularisation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var, coef in zip(feature_cols, \n",
    "                    logreg.coef_[0]):\n",
    "    print('%7s : %7.3f' %(var, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), feature_cols)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "plot_feature_importance(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build 5 trees\n",
    "forest = RandomForestClassifier(n_estimators=10, \n",
    "                                max_features=\"sqrt\", \n",
    "                                max_depth=None, \n",
    "                                min_samples_split=2,\n",
    "                                bootstrap=True)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"\\nAccuracy Score:\", format(accuracy_score(y_test, y_pred)))\n",
    "print(\"\\nClassificatin Report:\\n\", format(classification_report(y_test, y_pred)))\n",
    "print(\"Training set score: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"\\nTest set score: {:.3f}\".format(forest.score(X_test, y_test)))\n",
    "\n",
    "plot_feature_importance(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "\n",
    "mglearn.plots.plot_2d_seperator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
    "                               alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
